{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Настройка окружения"
      ],
      "metadata": {
        "id": "oYEy5xZMa6sk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qYX1n8RrCc-F"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4Fm3IrsLDBcB"
      },
      "outputs": [],
      "source": [
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kAowd2J-DSyN"
      },
      "outputs": [],
      "source": [
        "!tar xvzf spark-3.2.1-bin-hadoop3.2.tgz > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dfdhcirkGDcj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.2.1/spark-avro_2.12-3.2.1.jar  -P $SPARK_HOME/jars/\n",
        "\n",
        "!echo spark.executor.extraClassPath $SPARK_HOME/jars/spark-avro_2.12/3.2.1.jar\n",
        "\n",
        "!echo spark.driver.extraClassPath $SPARK_HOME/jars/spark-avro_2.12/3.2.1.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPsTNWv4Lkhi",
        "outputId": "5e9f2320-6834-4827-cd4a-bcec91f92803"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-19 07:46:41--  https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.2.1/spark-avro_2.12-3.2.1.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 186483 (182K) [application/java-archive]\n",
            "Saving to: ‘/content/spark-3.2.1-bin-hadoop3.2/jars/spark-avro_2.12-3.2.1.jar.9’\n",
            "\n",
            "\r          spark-avr   0%[                    ]       0  --.-KB/s               \rspark-avro_2.12-3.2 100%[===================>] 182.11K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-01-19 07:46:41 (5.48 MB/s) - ‘/content/spark-3.2.1-bin-hadoop3.2/jars/spark-avro_2.12-3.2.1.jar.9’ saved [186483/186483]\n",
            "\n",
            "spark.executor.extraClassPath /content/spark-3.2.1-bin-hadoop3.2/jars/spark-avro_2.12/3.2.1.jar\n",
            "spark.driver.extraClassPath /content/spark-3.2.1-bin-hadoop3.2/jars/spark-avro_2.12/3.2.1.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.2.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJTlMPL2qbin",
        "outputId": "b7eb9856-0cdb-4bce-9ad5-ca9f6d39268b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.2.1 in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.2.1) (0.10.9.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0-AdX-zFqSi",
        "outputId": "b378e5fa-aed6-4ec0-bdb6-899152d39dcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HauZPaywJs98",
        "outputId": "37477f58-f343-4c24-dadb-93b378dbb662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spark-3.2.1-bin-hadoop3.2\n"
          ]
        }
      ],
      "source": [
        "!echo $SPARK_HOME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SjDRZ0vTHbQT"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "import os\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "from pyspark import  SparkContext\n",
        "from pyspark.sql import functions as f\n",
        "appName = \"PySpark Task 1\"\n",
        "#master = \"spark://10.3.100.4:7077\"\n",
        "master = 'local[*]'\n",
        "# Create Spark session with Hive supported.\n",
        "spark = SparkSession.builder \\\n",
        "    .enableHiveSupport() \\\n",
        "    .appName(appName) \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтение исходных данных"
      ],
      "metadata": {
        "id": "ARqMS-0bbING"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvyKLVKgpY4j",
        "outputId": "48d8b6a8-357b-489a-cd98-ff00cdcc4c7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- COUNTRY_ID: string (nullable = true)\n",
            " |-- COUNTRY_NAME: string (nullable = true)\n",
            " |-- REGION_ID: integer (nullable = true)\n",
            "\n",
            "+----------+------------+---------+\n",
            "|COUNTRY_ID|COUNTRY_NAME|REGION_ID|\n",
            "+----------+------------+---------+\n",
            "|        AR|   Argentina|        2|\n",
            "|        AU|   Australia|        3|\n",
            "|        BE|     Belgium|        1|\n",
            "|        BR|      Brazil|        2|\n",
            "|        CA|      Canada|        2|\n",
            "+----------+------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "PATH = 'data_for_exam/countries'\n",
        "countries = spark.read.orc(PATH)\n",
        "\n",
        "countries = spark.read.orc(PATH)\n",
        "countries.printSchema()\n",
        "countries.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWguPKIcpY4k",
        "outputId": "851fe3ef-0eb0-42cf-f94e-dbf878ede05f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- DEPARTMENT_ID: integer (nullable = true)\n",
            " |-- DEPARTMENT_NAME: string (nullable = true)\n",
            " |-- MANAGER_ID: integer (nullable = true)\n",
            " |-- LOCATION_ID: integer (nullable = true)\n",
            "\n",
            "\n",
            "+-------------+---------------+----------+-----------+\n",
            "|DEPARTMENT_ID|DEPARTMENT_NAME|MANAGER_ID|LOCATION_ID|\n",
            "+-------------+---------------+----------+-----------+\n",
            "|           10| Administration|       200|       1700|\n",
            "|           20|      Marketing|       201|       1800|\n",
            "|           30|     Purchasing|       114|       1700|\n",
            "|           40|Human Resources|       203|       2400|\n",
            "|           50|       Shipping|       121|       1500|\n",
            "+-------------+---------------+----------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "PATH = 'data_for_exam/departments/departments'\n",
        "\n",
        "departments = spark.read\\\n",
        ".option('sep', ',')\\\n",
        ".option('header', 'True')\\\n",
        ".option('inferSchema', True)\\\n",
        ".csv(PATH)\n",
        "\n",
        "departments.printSchema()\n",
        "print()\n",
        "departments.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUwdUhKmpY4k",
        "outputId": "90f86759-545a-44db-9111-997431f4ccf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- EMPLOYEE_ID: integer (nullable = true)\n",
            " |-- FIRST_NAME: string (nullable = true)\n",
            " |-- LAST_NAME: string (nullable = true)\n",
            " |-- EMAIL: string (nullable = true)\n",
            " |-- PHONE_NUMBER: string (nullable = true)\n",
            " |-- HIRE_DATE: string (nullable = true)\n",
            " |-- JOB_ID: string (nullable = true)\n",
            " |-- SALARY: integer (nullable = true)\n",
            " |-- COMMISSION_PCT: string (nullable = true)\n",
            " |-- MANAGER_ID: integer (nullable = true)\n",
            " |-- DEPARTMENT_ID: integer (nullable = true)\n",
            "\n",
            "\n",
            "+-----------+----------+---------+--------+------------+---------+-------+------+--------------+----------+-------------+\n",
            "|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE| JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n",
            "+-----------+----------+---------+--------+------------+---------+-------+------+--------------+----------+-------------+\n",
            "|        100|    Steven|     King|   SKING|515.123.4567| 17.06.03|AD_PRES| 24000|          null|      null|           90|\n",
            "|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568| 21.09.05|  AD_VP| 17000|          null|       100|           90|\n",
            "|        102|       Lex|  De Haan| LDEHAAN|515.123.4569| 13.01.01|  AD_VP| 17000|          null|       100|           90|\n",
            "|        103| Alexander|   Hunold| AHUNOLD|590.423.4567| 03.01.06|IT_PROG|  9000|          null|       102|           60|\n",
            "|        104|     Bruce|    Ernst|  BERNST|590.423.4568| 21.05.07|IT_PROG|  6000|          null|       103|           60|\n",
            "+-----------+----------+---------+--------+------------+---------+-------+------+--------------+----------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "PATH = 'data_for_exam/employees/employees'\n",
        "\n",
        "employees = spark.read\\\n",
        ".option('sep', '\\t')\\\n",
        ".option('header', 'True')\\\n",
        ".option('inferSchema', True)\\\n",
        ".csv(PATH)\n",
        "\n",
        "employees.printSchema()\n",
        "print()\n",
        "employees.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNLWEHyMpY4k",
        "outputId": "a00f3a3b-9b27-4869-c04e-418d1a7d59f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- department_id: long (nullable = true)\n",
            " |-- employee_id: long (nullable = true)\n",
            " |-- end_date: string (nullable = true)\n",
            " |-- job_id: string (nullable = true)\n",
            " |-- start_date: string (nullable = true)\n",
            "\n",
            "\n",
            "+-------------+-----------+--------+----------+----------+\n",
            "|department_id|employee_id|end_date|    job_id|start_date|\n",
            "+-------------+-----------+--------+----------+----------+\n",
            "|           60|        102|24.07.06|   IT_PROG|  13.01.01|\n",
            "|          110|        101|27.10.01|AC_ACCOUNT|  21.09.97|\n",
            "|          110|        101|15.03.05|    AC_MGR|  28.10.01|\n",
            "|           20|        201|19.12.07|    MK_REP|  17.02.04|\n",
            "|           50|        114|31.12.07|  ST_CLERK|  24.03.06|\n",
            "+-------------+-----------+--------+----------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "PATH = 'data_for_exam/job_history/job_history'\n",
        "\n",
        "job_history = spark.read.json(PATH)\n",
        "\n",
        "job_history.printSchema()\n",
        "print()\n",
        "job_history.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpoFPaObPv-z",
        "outputId": "e84db4c7-42e4-4f18-c215-36e63716e3d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- JOB_ID: string (nullable = true)\n",
            " |-- JOB_TITLE: string (nullable = true)\n",
            " |-- MIN_SALARY: integer (nullable = true)\n",
            " |-- MAX_SALARY: integer (nullable = true)\n",
            "\n",
            "\n",
            "+----------+--------------------+----------+----------+\n",
            "|    JOB_ID|           JOB_TITLE|MIN_SALARY|MAX_SALARY|\n",
            "+----------+--------------------+----------+----------+\n",
            "|   AD_PRES|           President|     20080|     40000|\n",
            "|     AD_VP|Administration Vi...|     15000|     30000|\n",
            "|   AD_ASST|Administration As...|      3000|      6000|\n",
            "|    FI_MGR|     Finance Manager|      8200|     16000|\n",
            "|FI_ACCOUNT|          Accountant|      4200|      9000|\n",
            "+----------+--------------------+----------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "PATH = 'data_for_exam/jobs/jobs'\n",
        "\n",
        "jobs = spark.read\\\n",
        ".option('sep', ';')\\\n",
        ".option('header', 'True')\\\n",
        ".option('inferSchema', True)\\\n",
        ".csv(PATH)\n",
        "\n",
        "jobs.printSchema()\n",
        "print()\n",
        "jobs.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amvjBKaSpY4l",
        "outputId": "1af2a5df-a766-4069-b338-00956295631e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- LOCATION_ID: integer (nullable = true)\n",
            " |-- STREET_ADDRESS: string (nullable = true)\n",
            " |-- POSTAL_CODE: string (nullable = true)\n",
            " |-- CITY: string (nullable = true)\n",
            " |-- STATE_PROVINCE: string (nullable = true)\n",
            " |-- COUNTRY_ID: string (nullable = true)\n",
            "\n",
            "+-----------+--------------------+-----------+---------+----------------+----------+\n",
            "|LOCATION_ID|      STREET_ADDRESS|POSTAL_CODE|     CITY|  STATE_PROVINCE|COUNTRY_ID|\n",
            "+-----------+--------------------+-----------+---------+----------------+----------+\n",
            "|       1000|1297 Via Cola di Rie|      00989|     Roma|            null|        IT|\n",
            "|       1100|93091 Calle della...|      10934|   Venice|            null|        IT|\n",
            "|       1200|    2017 Shinjuku-ku|       1689|    Tokyo|Tokyo Prefecture|        JP|\n",
            "|       1300|     9450 Kamiya-cho|       6823|Hiroshima|            null|        JP|\n",
            "|       1400| 2014 Jabberwocky Rd|      26192|Southlake|           Texas|        US|\n",
            "+-----------+--------------------+-----------+---------+----------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "PATH = 'data_for_exam/locations'\n",
        "\n",
        "locations = spark.read.parquet(PATH)\n",
        "locations.printSchema()\n",
        "locations.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HEpOwWf6pY4l"
      },
      "outputs": [],
      "source": [
        "employees.createOrReplaceTempView('employees')\n",
        "countries.createOrReplaceTempView('countries')\n",
        "locations.createOrReplaceTempView('locations')\n",
        "departments.createOrReplaceTempView('departments')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функции для чтения и записи файлов Spark"
      ],
      "metadata": {
        "id": "EAi6Rrbda5Ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spark_write(df, format, compression, path):\n",
        "  df.write\\\n",
        "  .format(format)\\\n",
        "  .option('compression', compression)\\\n",
        "  .mode('overwrite')\\\n",
        "  .save(path)\n",
        "\n",
        "def spark_read(format, compression, path):\n",
        "  spark.read.format(format)\\\n",
        "  .option('compression', compression)\\\n",
        "  .load(path)\\\n",
        "  .show(5, False)"
      ],
      "metadata": {
        "id": "KngkG3mnazN9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Напишите запрос, возвращающий строку следующего вида для каждого сотрудника: \"<фамилия> зарабатывает <зарплата> каждый месяц, но хочет получать <зарплата * 3>\". Назовите колонку 'Dream Salaries'. Результат сохранить в формате parquet со сжатием GZIP"
      ],
      "metadata": {
        "id": "ReW3-VHMK2Bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY = '''\n",
        "select last_name || ' зарабатывает ' || salary || ' каждый месяц, но хочет получать '|| salary * 3\n",
        "as Dream_Salaries\n",
        "from employees\n",
        "'''\n",
        "\n",
        "df = spark.sql(QUERY)\n",
        "\n",
        "spark_write(df, 'parquet', 'gzip', 'results/task1')\n",
        "spark_read('parquet', 'gzip', 'results/task1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2weanY4e3YP",
        "outputId": "80f66ccf-a62e-4c5f-eac4-ab0126723752"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------+\n",
            "|Dream_Salaries                                                  |\n",
            "+----------------------------------------------------------------+\n",
            "|King зарабатывает 24000 каждый месяц, но хочет получать 72000   |\n",
            "|Kochhar зарабатывает 17000 каждый месяц, но хочет получать 51000|\n",
            "|De Haan зарабатывает 17000 каждый месяц, но хочет получать 51000|\n",
            "|Hunold зарабатывает 9000 каждый месяц, но хочет получать 27000  |\n",
            "|Ernst зарабатывает 6000 каждый месяц, но хочет получать 18000   |\n",
            "+----------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Напишите запрос, возвращающий адреса всех департаментов. Запрос должен возвращать ID локации, адрес (улицу), город, штат и страну."
      ],
      "metadata": {
        "id": "af5-3PL9LF6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY = '''\n",
        "select location_id, street_address, city, state_province, country_name\n",
        "from departments\n",
        "natural join locations\n",
        "natural join countries\n",
        "order by location_id\n",
        "'''\n",
        "\n",
        "df = spark.sql(QUERY)\n",
        "\n",
        "spark_write(df, 'parquet', 'gzip', 'results/task2')\n",
        "spark_read('parquet', 'gzip', 'results/task2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EQaVFIAfKLn",
        "outputId": "b7c33e9d-adc8-41d0-b04c-af6f28923565"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------------+-------------------+--------------+------------------------+\n",
            "|location_id|street_address     |city               |state_province|country_name            |\n",
            "+-----------+-------------------+-------------------+--------------+------------------------+\n",
            "|1400       |2014 Jabberwocky Rd|Southlake          |Texas         |United States of America|\n",
            "|1500       |2011 Interiors Blvd|South San Francisco|California    |United States of America|\n",
            "|1700       |2004 Charade Rd    |Seattle            |Washington    |United States of America|\n",
            "|1700       |2004 Charade Rd    |Seattle            |Washington    |United States of America|\n",
            "|1700       |2004 Charade Rd    |Seattle            |Washington    |United States of America|\n",
            "+-----------+-------------------+-------------------+--------------+------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишите запрос, возвращающий фамилию, ID отдела и наименование отдела для каждого сотрудника; Результат сохранить в формате avro со сжатием GZIP\n",
        "\n",
        "(прим. avro не поддерживает gzip)"
      ],
      "metadata": {
        "id": "indRJoSTLM_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY = '''\n",
        "select last_name, department_id, department_name\n",
        "from employees\n",
        "natural join departments\n",
        "order by last_name\n",
        "'''\n",
        "\n",
        "df = spark.sql(QUERY)\n",
        "\n",
        "spark_write(df, 'avro', 'deflate', 'results/task3')\n",
        "spark_read('avro', 'deflate', 'results/task3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk6ACZ42fa2h",
        "outputId": "9855a6b0-e9d6-4377-fdff-b8025bfcef19"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+---------------+\n",
            "|last_name|department_id|department_name|\n",
            "+---------+-------------+---------------+\n",
            "|Atkinson |50           |Shipping       |\n",
            "|Austin   |60           |IT             |\n",
            "|Baida    |30           |Purchasing     |\n",
            "|Bernstein|80           |Sales          |\n",
            "|Bissot   |50           |Shipping       |\n",
            "+---------+-------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Напишите запрос, возвращающий фамилию, ID сотрудника, фамилию менеджера и ID менеджера для каждого сотрудника (для сотрудников, у которых нет менеджера, в этих колонках должен быть NULL).\n",
        "Назовите колонки 'Employee', 'Emp#', 'Manager', 'Mgr#'. Результат сохранить в формате avro со сжатием Snappy"
      ],
      "metadata": {
        "id": "1RXJ9NCsRzkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY = '''\n",
        "select e.last_name as Employee, e.employee_id as Emp_N, m.last_name as Manager, e.manager_id as Mgr_N\n",
        "from employees as e\n",
        "left join employees as m on e.manager_id = m.employee_id\n",
        "order by e.employee_id\n",
        "'''\n",
        "\n",
        "df = spark.sql(QUERY)\n",
        "\n",
        "spark_write(df, 'avro', 'snappy', 'results/task4')\n",
        "spark_read('avro', 'snappy', 'results/task4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2rp0VhvfxHT",
        "outputId": "46576371-1e00-463a-ab21-7dd809df07cc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+-------+-----+\n",
            "|Employee|Emp_N|Manager|Mgr_N|\n",
            "+--------+-----+-------+-----+\n",
            "|King    |100  |null   |null |\n",
            "|Kochhar |101  |King   |100  |\n",
            "|De Haan |102  |King   |100  |\n",
            "|Hunold  |103  |De Haan|102  |\n",
            "|Ernst   |104  |Hunold |103  |\n",
            "+--------+-----+-------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Напишите запрос, возвращающий фамилии и зарплаты всех сотрудников, которые подчиняются сотруднику King. Используйте подзапрос."
      ],
      "metadata": {
        "id": "hF2W5oODSPWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY = '''\n",
        "select last_name, salary\n",
        "from employees\n",
        "where manager_id in (select employee_id\n",
        "                     from employees\n",
        "                     where last_name = 'King')\n",
        "'''\n",
        "\n",
        "df = spark.sql(QUERY)\n",
        "\n",
        "spark_write(df, 'parquet', 'gzip', 'results/task5')\n",
        "spark_read('parquet', 'gzip', 'results/task5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Do63HyzgWOL",
        "outputId": "957b22b0-6ca5-4182-8c97-c135b8bfdcc3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+\n",
            "|last_name|salary|\n",
            "+---------+------+\n",
            "|Kochhar  |17000 |\n",
            "|De Haan  |17000 |\n",
            "|Raphaely |11000 |\n",
            "|Weiss    |8000  |\n",
            "|Fripp    |8200  |\n",
            "+---------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Напишите запрос, возвращающий фамилии всех сотрудников, получающих больше, чем любой сотрудник отдела с ID 60."
      ],
      "metadata": {
        "id": "tqkiAlPASbEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY = '''\n",
        "select last_name\n",
        "from employees\n",
        "where salary > (select max(salary)\n",
        "\t\t\t\t        from employees\n",
        "\t\t\t\t        where department_id = 60)\n",
        "'''\n",
        "df = spark.sql(QUERY)\n",
        "\n",
        "spark_write(df, 'parquet', 'gzip', 'results/task6')\n",
        "spark_read('parquet', 'gzip', 'results/task6')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZTtKiRUgmLi",
        "outputId": "ae948fd8-5ba3-4658-fba0-2ca70f5b6e93"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|last_name|\n",
            "+---------+\n",
            "|King     |\n",
            "|Kochhar  |\n",
            "|De Haan  |\n",
            "|Greenberg|\n",
            "|Raphaely |\n",
            "+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Напишите запрос, возвращающий ID, фамилии и зарплаты всех сотрудников, работающих в одном отделе с работником, в чьей фамилии есть буква 'u' и получающих больше средней зарплаты в компании."
      ],
      "metadata": {
        "id": "NnzyUZWCSk2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY = '''\n",
        "select employee_id, last_name\n",
        "from employees\n",
        "where department_id in (select distinct(department_id)\n",
        "\t\t\t\t\t\t            from employees\n",
        "\t\t\t\t\t\t            where last_name like '%u%')\n",
        "and employee_id not in (select employee_id\n",
        "\t\t\t\t\t\t            from employees\n",
        "\t\t\t\t\t\t            where last_name like '%u%')\n",
        "and salary > (select avg(salary)\n",
        "\t\t\t        from employees)\n",
        "order by salary\n",
        "'''\n",
        "\n",
        "df = spark.sql(QUERY)\n",
        "\n",
        "spark_write(df, 'parquet', 'gzip', 'results/task7')\n",
        "spark_read('parquet', 'gzip', 'results/task7')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyneqzdFYoWV",
        "outputId": "5d87e843-f3d9-4698-88eb-1a546dbc806f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+\n",
            "|employee_id|last_name|\n",
            "+-----------+---------+\n",
            "|123        |Vollman  |\n",
            "|165        |Lee      |\n",
            "|161        |Sewall   |\n",
            "|164        |Marvins  |\n",
            "|172        |Bates    |\n",
            "+-----------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Выведите фамилии, id отдела и название отдела для всех сотрудников, не привязанных ни к одному отделу, а также список отделов, к которым не привязан ни один сотрудник."
      ],
      "metadata": {
        "id": "VL_9b-niS4Fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY = '''\n",
        "select last_name, employees.department_id, department_name\n",
        "from employees\n",
        "left join departments on employees.department_id = departments.department_id\n",
        "where employees.department_id is null\n",
        "\n",
        "union\n",
        "\n",
        "select last_name, employees.department_id, department_name\n",
        "from departments\n",
        "left join employees on employees.department_id = departments.department_id\n",
        "where employees.last_name is null\n",
        "'''\n",
        "\n",
        "df = spark.sql(QUERY)\n",
        "\n",
        "spark_write(df, 'parquet', 'gzip', 'results/task8')\n",
        "spark_read('parquet', 'gzip', 'results/task8')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmd6mLNEg-FJ",
        "outputId": "c61e06b4-a326-46ca-be0d-8ab7472c796c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+----------------+\n",
            "|last_name|department_id|department_name |\n",
            "+---------+-------------+----------------+\n",
            "|Grant    |null         |null            |\n",
            "|null     |null         |Corporate Tax   |\n",
            "|null     |null         |Government Sales|\n",
            "|null     |null         |Payroll         |\n",
            "|null     |null         |Recruiting      |\n",
            "+---------+-------------+----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0OUTUxCXpY4n"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}